#Basic code which ignores live webcam trainings! The real code is the other one.

!wget https://www.dropbox.com/s/nilt43hyl1dx82k/dataset.zip?dl=0

!unzip dataset.zip?dl=0

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from keras.layers import Flatten, Dense
from keras.models import Model
from keras.preprocessing.image import ImageDataGenerator , img_to_array, load_img
from keras.applications.mobilenet import MobileNet, preprocess_input 
from keras.losses import categorical_crossentropy

# Working with pre trained model 

base_model = MobileNet( input_shape=(224,224,3), include_top= False )

for layer in base_model.layers:
  layer.trainable = False


x = Flatten()(base_model.output)
x = Dense(units=7 , activation='softmax' )(x)

# creating our model.
model = Model(base_model.input, x)

model.compile(optimizer='adam', loss= categorical_crossentropy , metrics=['accuracy']  )


train_datagen = ImageDataGenerator(
     zoom_range = 0.2, 
     shear_range = 0.2, 
     horizontal_flip=True, 
     rescale = 1./255
)

train_data = train_datagen.flow_from_directory(directory= "/content/train", 
                                               target_size=(224,224), 
                                               batch_size=32,
                                  )


train_data.class_indices

val_datagen = ImageDataGenerator(rescale = 1./255 )

val_data = val_datagen.flow_from_directory(directory= "/content/test", 
                                           target_size=(224,224), 
                                           batch_size=32,
                                  )

## having early stopping and model check point 

from keras.callbacks import ModelCheckpoint, EarlyStopping

# early stopping
es = EarlyStopping(monitor='val_accuracy', min_delta= 0.01 , patience= 5, verbose= 1, mode='auto')

# model check point
mc = ModelCheckpoint(filepath="best_model.h5", monitor= 'val_accuracy', verbose= 1, save_best_only= True, mode = 'auto')

# puting call back in a list 
call_back = [es, mc]

hist = model.fit_generator(train_data, 
                           steps_per_epoch= 20, 
                           epochs= 30, 
                           validation_data= val_data, 
                           validation_steps= 8, 
                           callbacks=[es,mc])

# Loading the best fit model 
from keras.models import load_model
model = load_model("/content/best_model.h5")

h =  hist.history
h.keys()


# just to map o/p values 
op = dict(zip( train_data.class_indices.values(), train_data.class_indices.keys()))











import cv2
import numpy as np
from keras.preprocessing import image
from google.colab import files
from IPython.display import display, Javascript
from google.colab.output import eval_js
import base64
import io
from PIL import Image
import warnings
warnings.filterwarnings("ignore")
from keras.models import load_model
from google.colab.patches import cv2_imshow

# Load the model
model = load_model("best_model.h5")

face_haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')

# Function to capture a video frame using webcam in Colab
def take_photo(filename='photo.jpg', quality=0.8):
    js = Javascript('''
        async function takePhoto(quality) {
            const div = document.createElement('div');
            const capture = document.createElement('button');
            capture.textContent = 'Capture';
            div.appendChild(capture);

            const video = document.createElement('video');
            video.style.display = 'block';
            const stream = await navigator.mediaDevices.getUserMedia({ 'video': true });

            document.body.appendChild(div);
            div.appendChild(video);
            video.srcObject = stream;
            await video.play();

            // Resize the output to fit the video element.
            google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);

            // Wait for Capture to be clicked.
            await new Promise((resolve) => capture.onclick = resolve);

            const canvas = document.createElement('canvas');
            canvas.width = video.videoWidth;
            canvas.height = video.videoHeight;
            canvas.getContext('2d').drawImage(video, 0, 0);
            stream.getVideoTracks()[0].stop();
            div.remove();
            return canvas.toDataURL('image/jpeg', quality);
        }
    ''')
    display(js)
    data = eval_js('takePhoto({})'.format(quality))
    binary = base64.b64decode(data.split(',')[1])
    with open(filename, 'wb') as f:
        f.write(binary)
    return filename

# Capture a photo using the webcam
take_photo()

# Read the captured image
test_img = cv2.imread('photo.jpg')
gray_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)

faces_detected = face_haar_cascade.detectMultiScale(gray_img, 1.32, 5)

for (x, y, w, h) in faces_detected:
    cv2.rectangle(test_img, (x, y), (x + w, y + h), (255, 0, 0), thickness=7)
    roi_gray = gray_img[y:y + w, x:x + h]
    roi_gray = cv2.resize(roi_gray, (224, 224))
    img_pixels = image.img_to_array(roi_gray)
    img_pixels = np.expand_dims(img_pixels, axis=0)
    img_pixels /= 255

    predictions = model.predict(img_pixels)

    max_index = np.argmax(predictions[0])

    # Print the predicted class index
    print("Max Index:", max_index)

    emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')
    predicted_emotion = emotions[max_index]

    # Ensure that max_index is within the range of emotions tuple
    if 0 <= max_index < len(emotions):
        predicted_emotion = emotions[max_index]
        # Print the predicted emotion
        print("Predicted Emotion from the Face Detection:", predicted_emotion)
        cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
    else:
        print("Error: Index out of range for emotions tuple.")

resized_img = cv2.resize(test_img, (1000, 700))
cv2_imshow(resized_img)
cv2.waitKey(0)
cv2.destroyAllWindows()









from IPython.display import HTML

# Define a dictionary mapping emotions to YouTube video links
emotion_video_dict = {
        'angry': 'https://www.youtube.com/watch?v=25DC6v8oPxo',
    'disgust': 'https://www.youtube.com/watch?v=1zEIuTPho34&list=RDGMEMQ1dJ7wXfLlqCjwV0xfSNbAVM1zEIuTPho34&start_radio=1',
    'fear': 'https://www.youtube.com/watch?v=peLRPSCeppI',
    'happy': 'https://www.youtube.com/watch?v=ZbZSe6N_BXs',
    'sad': 'https://www.youtube.com/watch?v=xpCl_P03AaA',
    'surprise': 'https://www.youtube.com/watch?v=yuFI5KSPAt4',
    'neutral': 'https://www.youtube.com/watch?v=dvgZkm1xWPE&list=RDGMEMQ1dJ7wXfLlqCjwV0xfSNbAVM1zEIuTPho34&index=18'
}

# Print the predicted class index
print("Max Index:", max_index)

emotions = ('angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral')
predicted_emotion = emotions[max_index]

# Ensure that max_index is within the range of emotions tuple
if 0 <= max_index < len(emotions):
    predicted_emotion = emotions[max_index]
    # Print the predicted emotion
    print("Predicted Emotion from the Face Detection:", predicted_emotion)
    cv2.putText(test_img, predicted_emotion, (int(x), int(y)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)

    # Get the YouTube video link based on the predicted emotion
    youtube_video_link = emotion_video_dict.get(predicted_emotion, 'https://www.youtube.com/watch?v=default_youtube_video_link')
    print("Recommended YouTube Video Link:", youtube_video_link)

    # Display a clickable link to the YouTube video
    display(HTML(f'<a href="{youtube_video_link}" target="_blank">Click here to watch the recommended YouTube video</a>'))
else:
    print("Error: Index out of range for emotions tuple.")
    youtube_video_link = 'https://www.youtube.com/watch?v=default_youtube_video_link'
